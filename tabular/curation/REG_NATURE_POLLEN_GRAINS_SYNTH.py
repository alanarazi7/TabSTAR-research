from tabular.datasets.manual_curation_obj import CuratedFeature, CuratedTarget
from tabular.preprocessing.objects import SupervisedTask

'''
Dataset Name: pollen
====
Examples: 3848
====
URL: https://www.openml.org/search?type=data&id=529
====
Description: **Author**:   
**Source**: Unknown - Date unknown  
**Please cite**:   

This dataset is synthetic.  It was generated by David Coleman
at RCA Laboratories in Princeton, N.J.  For convenience, we will
refer to it as the POLLEN DATA.  The first three variables are the
lengths of geometric features observed sampled pollen grains - in the
x, y, and z dimensions: a "ridge" along x, a "nub" in the y
direction, and a "crack" in along the z dimension.  The fourth
variable is pollen grain weight, and the fifth is density.

There are 3848 observations, in random order (for people whose
software packages cannot handle this much data, it is recommended
that the data be sampled).  The dataset is broken up into eight
pieces, POLLEN1.DAT - POLLEN8.DAT, each with 481 observations.
We will call the variables:

1. RIDGE
2. NUB
3. CRACK
4. WEIGHT
5. DENSITY

6. OBSERVATION NUMBER (for convenience)

The data analyst is advised that there is more than one "feature" to
these data.  Each feature can be observed through various graphical
techniques, but analytic methods, as well, can help "crack" the
dataset.

Additional Info:

I no longer have the description handed out during the JSM, but can
tell you how I generated the data, in minitab.

1. Part A was generated: 5000 (I think) 5-variable, uncorrelated, i.i.d.
Gaussian observations.

2. To get part B, I duplicated part A, then reversed the sign on the
observations for 3 of the 5 variables.

3. Part B was appended to Part A.

4. The order of the observations was randomized.

5. While waiting for my tardy car-pool companion, I took a piece of
graph paper, and figured out a dot-matrix representation of the word,
"EUREKA."  I then added these observations to the "center" of the
datatset.

6. The data were scaled, by variable (something like 1,3,5,7,11).

7. The data were rotated, then translated.

8. A few points in space within the datacloud were chosen as ellipsoid
centers, then for each center, all observations within a (scaled and
rotated) radius were identified, and eliminated - to form ellipsoidal
voids.

9. The variables were given entirely ficticious names.

FYI, only the folks at Bell Labs, Murray Hill, found everything,
including the voids.

Hope this is helpful!

References:

Becker, R.A., Denby, L., McGill, R., and Wilks,
A. (1986). Datacryptanalysis: A Case Study.
Proceedings of the Section on Statistical Graphics, 92-97.

Slomka, M. (1986). The Analysis of a Synthetic Data Set.
Proceedings of the Section on Statistical Graphics, 113-116.



Information about the dataset
CLASSTYPE: numeric
CLASSINDEX: none specific
====
Target Variable: DENSITY (numeric, 3784 distinct): ['0.3398', '-0.5923', '3.6757', '0.1951', '-1.9897', '-2.3824', '-3.5495', '-3.5095', '-2.1129', '-2.9398']
====
Features:

RIDGE (numeric, 3809 distinct): ['-4.7204', '4.8729', '-0.7744', '4.0982', '5.3055', '4.6631', '-0.874', '-6.3546', '-1.3294', '-12.5226']
NUB (numeric, 3811 distinct): ['-5.5403', '-3.7632', '-1.3572', '-2.8578', '-3.4128', '4.9431', '-1.0886', '2.9382', '2.0574', '7.7824']
CRACK (numeric, 3816 distinct): ['-2.3679', '3.8756', '1.2378', '4.3293', '0.6998', '-0.4408', '-1.1699', '-1.1368', '1.1927', '-3.9651']
WEIGHT (numeric, 3826 distinct): ['11.4315', '0.8551', '13.8763', '2.5895', '14.8641', '-1.8943', '-1.2819', '-8.2595', '4.5186', '1.5782']
'''

CONTEXT = "Pollen Grains Concentration"
TARGET = CuratedTarget(raw_name="DENSITY", task_type=SupervisedTask.REGRESSION)
COLS_TO_DROP = []
FEATURES = []